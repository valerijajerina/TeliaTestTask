{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution of kaggle competition for Telia practice\n",
    "\n",
    "Competition link: https://www.kaggle.com/competitions/nlp-getting-started\n",
    "\n",
    "Author: Valerija Jerina"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tokenization\n",
    "from wordcloud import STOPWORDS\n",
    "import transformers\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "import random\n",
    "\n",
    "random.seed(1201)\n",
    "df_train = pd.read_csv('kaggle/input/train.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "df_test = pd.read_csv('kaggle/input/test.csv', dtype={'id': np.int16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing in machine learning that we have to consider from the benning is how we treat missing values. \n",
    "\n",
    "Thus, i suggest that we take a look at the missing values ratio in training and test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of the mussed keywords in location in training set 0.024082116067903673\n",
      "Ratio of the mussed keywords in location in test set 0.023529411764705882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "values_train = df_train[['keyword', 'location']].isnull().sum()\n",
    "values_test = df_test[['keyword', 'location']].isnull().sum()\n",
    "\n",
    "print(\"Ratio of the mussed keywords in location in training set \" + str(values_train[0]/values_train[1]))\n",
    "print(\"Ratio of the mussed keywords in location in test set \" + str(values_test[0]/values_test[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, those ratios are very similar. We can assume that both sets has been taken from the same sample. \n",
    "\n",
    "Now, let's fill those values since we don't want to get rid of the data this easily. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['keyword'] = df_train['keyword'].fillna(f'no_keyword')\n",
    "df_train['location'] = df_train['location'].fillna(f'no_location')\n",
    "df_test['keyword'] = df_test['keyword'].fillna(f'no_keyword')\n",
    "df_test['location'] = df_test['location'].fillna(f'no_location')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we have to create some extra features to as much as we can from the data we use.\n",
    "\n",
    "In this case, as in the most language models, following things might matter:\n",
    "1. text length(number of words and characters in it)\n",
    "2. Number of unique words in text(debatable, but can affect the model)\n",
    "3. Hashtags(or their count, since we are watching tweets)\n",
    "4. Mentions in the text(as we have tweets where people can be taged using @)\n",
    "5. Stop words, is it a stop word and it's count. \n",
    "6. Average word length\n",
    "7. there is a possibility, that punctuation count is also impotant.\n",
    "8. Are there any links(urls)?\n",
    "\n",
    "Let's implement some of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character_count\n",
    "df_train['character_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "df_test['character_count'] = df_test['text'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amount of unique_words\n",
    "df_train['unique_words'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "df_test['unique_words'] = df_test['text'].apply(lambda x: len(set(str(x).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_word_count\n",
    "df_train['stop_words'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "df_test['stop_words'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['urls'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "df_test['urls'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "df_train['hashtags'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "df_test['hashtags'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "df_train['mentions'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "df_test['mentions'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stop at this point. Many more meta features can be added, however, if result is not satisfyiong, we can always add mor later"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target\n",
    "\n",
    "Now we have to check if the model will require extra targeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5703402075397347\n"
     ]
    }
   ],
   "source": [
    "not_disaster = df_train.groupby('target').count()['id'][0]\n",
    "disaster = df_train.groupby('target').count()['id'][1]\n",
    "print(not_disaster/(disaster+not_disaster))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a little above 57% are not disasters, that means that a little below 43% will be not disasters. The difference is not big enough for us to worry about stratification by target."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential steps:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, I am not completely sure whether i'll be able to complete all the ideas that i have for this dataset. Thus, i am suggesting a list of potential steps that could be done.\n",
    "\n",
    "* N-grams -- N-grams are continuous sequences of words or symbols or tokens in a document. We want to see whether any of those appear too often. \n",
    "* Embeddings coverage -- We want to get vocabulary as close to the embeddings as possible in order to not lose any important information\n",
    "* Text cleaning -- good models are trained on the clean data! First, we would want to separate words with punctuation, then we would consider removing special characters that appear in the words. Then, expanding contractions, removing urls, correcting slang and typos, changing informal words to full forms. Check for arconyms, replace those that can be replacesd, if possible, then expanding hashtags and usernames. Possible step is changing everything to lowercase, lemmatization.\n",
    "* Handling mislabeld samples - data contains duplicates and some of those can be interpreted differently. Thus, those have to ba handled manually.\n",
    "* Cross-Validation - stratification of the data by 'keyword'\n",
    "* Model training - getting accuracy, precision, trecall, since F1 by itself is not very informative. applying BERT layer if needed.\n",
    "* Train, Evaluate, Predict. \n",
    "* Test!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As i do not have that much time, I will not go into detail and explore N-Grams, however it would be beneficial. Thus, I will leave a code piece that would potentially help with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "In order to save time i went through embeddings that have been used by other submitters and chose one that i believe would suit the most - GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = np.load('kaggle/embeddings/glove.840B.300d.pkl', allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check for what is the coverage of vocabulary by this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: Vocabulary coverage = 0.5206, Text coverage = 0.8268\n",
      "Testing data: Vocabulary coverage = 0.5721, Text coverage = 0.8185\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate vocabulary coverage for a given set of text data\n",
    "def calculate_coverage(text, embeddings):\n",
    "    vocab = {}\n",
    "    for t in text:\n",
    "        for w in t:\n",
    "            try:\n",
    "                vocab[w] += 1\n",
    "            except KeyError:\n",
    "                vocab[w] = 1 \n",
    "\n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    \n",
    "    return vocab_coverage, text_coverage\n",
    "\n",
    "# Example usage\n",
    "train_text = df_train['text'].apply(lambda s: s.split()).values\n",
    "test_text = df_test['text'].apply(lambda s: s.split()).values\n",
    "\n",
    "train_vocab_coverage, train_text_coverage = calculate_coverage(train_text, glove_embeddings)\n",
    "test_vocab_coverage, test_text_coverage = calculate_coverage(test_text, glove_embeddings)\n",
    "\n",
    "print(f\"Training data: Vocabulary coverage = {train_vocab_coverage:.4f}, Text coverage = {train_text_coverage:.4f}\")\n",
    "print(f\"Testing data: Vocabulary coverage = {test_vocab_coverage:.4f}, Text coverage = {test_text_coverage:.4f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here i've realised that i won't manage to insert most of the special characters manually and take into consideration all the `\\x89Û_`, `\\x89Ûª`, `JapÌ_n`, etc.\n",
    "\n",
    "Same with hashtags, it'll take a very long time to clean all of it, so i'll just do some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text): \n",
    "    \n",
    "    # Contractions\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"We're\", \"We are\", text)\n",
    "    text = re.sub(r\"That's\", \"That is\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"they're\", \"they are\", text)\n",
    "    text = re.sub(r\"Can't\", \"Cannot\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"aren't\", \"are not\", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"What's\", \"What is\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "    text = re.sub(r\"There's\", \"There is\", text)\n",
    "    text = re.sub(r\"He's\", \"He is\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"You're\", \"You are\", text)\n",
    "    text = re.sub(r\"I'M\", \"I am\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "    text = re.sub(r\"i'm\", \"I am\", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\"Isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"Here's\", \"Here is\", text)\n",
    "    text = re.sub(r\"you've\", \"you have\", text)\n",
    "    text = re.sub(r\"we're\", \"we are\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"we've\", \"we have\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"y'all\", \"you all\", text)\n",
    "    text = re.sub(r\"would've\", \"would have\", text)\n",
    "    text = re.sub(r\"it'll\", \"it will\", text)\n",
    "    text = re.sub(r\"we'll\", \"we will\", text)\n",
    "    text = re.sub(r\"We've\", \"We have\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"Y'all\", \"You all\", text)\n",
    "    text = re.sub(r\"Weren't\", \"Were not\", text)\n",
    "    text = re.sub(r\"Didn't\", \"Did not\", text)\n",
    "    text = re.sub(r\"they'll\", \"they will\", text)\n",
    "    text = re.sub(r\"they'd\", \"they would\", text)\n",
    "    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
    "    text = re.sub(r\"they've\", \"they have\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"should've\", \"should have\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"we'd\", \"we would\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"weren't\", \"were not\", text)\n",
    "    text = re.sub(r\"They're\", \"They are\", text)\n",
    "    text = re.sub(r\"let's\", \"let us\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"you're\", \"you are\", text)\n",
    "    text = re.sub(r\"i've\", \"I have\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"I've\", \"I have\", text)\n",
    "    text = re.sub(r\"Don't\", \"do not\", text)\n",
    "    text = re.sub(r\"I'll\", \"I will\", text)\n",
    "    text = re.sub(r\"I'd\", \"I would\", text)\n",
    "    text = re.sub(r\"Let's\", \"Let us\", text)\n",
    "    text = re.sub(r\"you'd\", \"You would\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"Ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"Haven't\", \"Have not\", text)\n",
    "    text = re.sub(r\"Could've\", \"Could have\", text)\n",
    "    text = re.sub(r\"youve\", \"you have\", text)  \n",
    "            \n",
    "    # Character entity references\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)\n",
    "    \n",
    "    # Typos, slang and informal abbreviations\n",
    "    text = re.sub(r\"16yr\", \"16 year\", text)\n",
    "    text = re.sub(r\"w/e\", \"whatever\", text)\n",
    "    text = re.sub(r\"w/\", \"with\", text)\n",
    "    text = re.sub(r\"USAgov\", \"USA government\", text)\n",
    "    text = re.sub(r\"recentlu\", \"recently\", text)\n",
    "    text = re.sub(r\"TRAUMATISED\", \"traumatized\", text)\n",
    "    text = re.sub(r\"<3\", \"love\", text)\n",
    "    text = re.sub(r\"8/5/2015\", \"2015-08-05\", text)\n",
    "    text = re.sub(r\"8/6/2015\", \"2015-08-06\", text)\n",
    "    text = re.sub(r\"10:38PM\", \"10:38 PM\", text)\n",
    "    text = re.sub(r\"10:30pm\", \"10:30 PM\", text)\n",
    "    text = re.sub(r\"lmao\", \"laughing my ass off\", text)   \n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    text = re.sub(r\"IranDeal\", \"Iran Deal\", text)\n",
    "    text = re.sub(r\"ArianaGrande\", \"Ariana Grande\", text)\n",
    "    text = re.sub(r\"camilacabello97\", \"camila cabello\", text) \n",
    "    text = re.sub(r\"RondaRousey\", \"Ronda Rousey\", text)     \n",
    "    text = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", text)\n",
    "    text = re.sub(r\"PantherAttack\", \"Panther Attack\", text)\n",
    "    text = re.sub(r\"StrategicPatience\", \"Strategic Patience\", text)\n",
    "    text = re.sub(r\"socialnews\", \"social news\", text)\n",
    "    text = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", text)\n",
    "    text = re.sub(r\"humanconsumption\", \"human consumption\", text)\n",
    "    text = re.sub(r\"BeingAuthor\", \"Being Author\", text)\n",
    "    text = re.sub(r\"OffensiveContent\", \"Offensive Content\", text)\n",
    "    text = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", text)\n",
    "    text = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", text)\n",
    "    text = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", text)\n",
    "    text = re.sub(r\"NewsInTweets\", \"News In Tweets\", text)\n",
    "    text = re.sub(r\"abstorm\", \"Alberta Storm\", text)\n",
    "    text = re.sub(r\"Time2015\", \"Time 2015\", text)\n",
    "    text = re.sub(r\"djicemoon\", \"dj icemoon\", text)\n",
    "    text = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", text)\n",
    "    text = re.sub(r\"ENGvAUS\", \"England vs Australia\", text)\n",
    "    text = re.sub(r\"ScottWalker\", \"Scott Walker\", text)\n",
    "    text = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", text)\n",
    "    text = re.sub(r\"RAmag\", \"Royal Academy Magazine\", text)\n",
    "    text = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", text)\n",
    "    text = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", text)\n",
    "    text = re.sub(r\"GamerGate\", \"Gamer Gate\", text)\n",
    "    text = re.sub(r\"alexbelloli\", \"Alex Belloli\", text)\n",
    "    text = re.sub(r\"Japton\", \"Arkansas\", text)\n",
    "    text = re.sub(r\"timkaine\", \"Tim Kaine\", text)\n",
    "    text = re.sub(r\"IdentityTheft\", \"Identity Theft\", text)\n",
    "    text = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", text)\n",
    "    text = re.sub(r\"mishacollins\", \"Misha Collins\", text)\n",
    "    text = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", text)\n",
    "    text = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", text)\n",
    "    text = re.sub(r\"Kowing\", \"Knowing\", text)\n",
    "    text = re.sub(r\"ScreamQueens\", \"Scream Queens\", text)\n",
    "    text = re.sub(r\"AskCharley\", \"Ask Charley\", text)\n",
    "    text = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", text)\n",
    "    text = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", text)\n",
    "    text = re.sub(r\"HannaPH\", \"Typhoon Hanna\", text)\n",
    "    text = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", text)\n",
    "    text = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", text)\n",
    "    text = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", text)\n",
    "    text = re.sub(r\"Popularmmos\", \"Popular MMOs\", text)\n",
    "    text = re.sub(r\"WildHorses\", \"Wild Horses\", text)\n",
    "    text = re.sub(r\"FantasticFour\", \"Fantastic Four\", text)\n",
    "    text = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", text)\n",
    "    text = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", text)\n",
    "    text = re.sub(r\"residualincome\", \"residual income\", text)\n",
    "    text = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", text)\n",
    "    text = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", text)\n",
    "    text = re.sub(r\"AmazonDeals\", \"Amazon Deals\", text)\n",
    "    text = re.sub(r\"charlesadler\", \"Charles Adler\", text)\n",
    "    text = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", text)\n",
    "    text = re.sub(r\"txlege\", \"Texas Legislature\", text)\n",
    "    text = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", text)\n",
    "    text = re.sub(r\"Newss\", \"News\", text)\n",
    "    text = re.sub(r\"hempoil\", \"hemp oil\", text)\n",
    "    text = re.sub(r\"57am\", \"57 am\", text)\n",
    "    text = re.sub(r\"Bokoharm\", \"Boko Haram\", text)\n",
    "    text = re.sub(r\"BombEffects\", \"Bomb Effects\", text)\n",
    "    text = re.sub(r\"win10\", \"Windows 10\", text)\n",
    "    text = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", text)\n",
    "    text = re.sub(r\"pctool\", \"pc tool\", text)\n",
    "    text = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", text)\n",
    "    text = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", text)\n",
    "    text = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", text)\n",
    "    text = re.sub(r\"LakeEffect\", \"Lake Effect\", text)\n",
    "    text = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", text)\n",
    "    text = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", text)\n",
    "    text = re.sub(r\"writerslife\", \"writers life\", text)\n",
    "    text = re.sub(r\"NaturalBirth\", \"Natural Birth\", text)\n",
    "    text = re.sub(r\"UnusualWords\", \"Unusual Words\", text)\n",
    "    text = re.sub(r\"TheaterTrial\", \"Theater Trial\", text)\n",
    "    text = re.sub(r\"CatoInstitute\", \"Cato Institute\", text)\n",
    "    text = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", text)\n",
    "    text = re.sub(r\"uiseful\", \"useful\", text)\n",
    "    text = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", text)\n",
    "    text = re.sub(r\"kindlng\", \"kindling\", text)\n",
    "    text = re.sub(r\"riggd\", \"rigged\", text)\n",
    "    text = re.sub(r\"slownewsday\", \"slow news day\", text)\n",
    "    text = re.sub(r\"mortalkombat\", \"Mortal Kombat\", text)\n",
    "    text = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", text)\n",
    "    text = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", text)\n",
    "    text = re.sub(r\"kostumes\", \"costumes\", text)\n",
    "    text = re.sub(r\"YEEESSSS\", \"yes\", text)\n",
    "    text = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", text)\n",
    "    text = re.sub(r\"IntlDevelopment\", \"Intl Development\", text)\n",
    "    text = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", text)\n",
    "    text = re.sub(r\"NewsThousands\", \"News Thousands\", text)\n",
    "    text = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", text)\n",
    "    text = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", text)\n",
    "    text = re.sub(r\"FromTheField\", \"From the field\", text)\n",
    "    text = re.sub(r\"NorthIowa\", \"North Iowa\", text)\n",
    "    text = re.sub(r\"WillowFire\", \"Willow Fire\", text)\n",
    "    text = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", text)\n",
    "    text = re.sub(r\"viaYouTube\", \"via YouTube\", text)\n",
    "           \n",
    "    # Urls\n",
    "    text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text)\n",
    "\n",
    "    #punctuations and special char\n",
    "    punctuation = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuation:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    text = text.replace('...', ' ... ')\n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ') \n",
    "\n",
    "    #One acronym that i've found\n",
    "    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_cleaned'] = df_train['text'].apply(lambda s : clean(s))\n",
    "df_test['text_cleaned'] = df_test['text'].apply(lambda s : clean(s))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall see if we have improved :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: Vocabulary coverage = 0.8033, Text coverage = 0.9613\n",
      "Testing data: Vocabulary coverage = 0.8397, Text coverage = 0.9590\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate vocabulary coverage for a given set of text data\n",
    "def calculate_coverage(text, embeddings):\n",
    "    vocab = {}\n",
    "    for t in text:\n",
    "        for w in t:\n",
    "            try:\n",
    "                vocab[w] += 1\n",
    "            except KeyError:\n",
    "                vocab[w] = 1 \n",
    "\n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    \n",
    "    return vocab_coverage, text_coverage\n",
    "\n",
    "# Example usage\n",
    "train_text = df_train['text_cleaned'].apply(lambda s: s.split()).values\n",
    "test_text = df_test['text_cleaned'].apply(lambda s: s.split()).values\n",
    "\n",
    "train_vocab_coverage, train_text_coverage = calculate_coverage(train_text, glove_embeddings)\n",
    "test_vocab_coverage, test_text_coverage = calculate_coverage(test_text, glove_embeddings)\n",
    "\n",
    "print(f\"Training data: Vocabulary coverage = {train_vocab_coverage:.4f}, Text coverage = {train_text_coverage:.4f}\")\n",
    "print(f\"Testing data: Vocabulary coverage = {test_vocab_coverage:.4f}, Text coverage = {test_text_coverage:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling mislabeled samples\n",
    "first, let's take a look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit',\n",
       " 'Hellfire! We don\\x89Ûªt even want to think about it or mention it so let\\x89Ûªs not do anything that leads to it #islam!',\n",
       " \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\n",
       " 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!',\n",
       " 'To fight bioterrorism sir.',\n",
       " 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE',\n",
       " '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption',\n",
       " '#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect',\n",
       " 'He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam',\n",
       " 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG',\n",
       " 'Hellfire is surrounded by desires so be careful and don\\x89Ûªt let your desires control you! #Afterlife',\n",
       " 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring',\n",
       " \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\",\n",
       " 'wowo--=== 12000 Nigerian refugees repatriated from Cameroon',\n",
       " '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4',\n",
       " 'Caution: breathing may be hazardous to your health.',\n",
       " 'I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????',\n",
       " 'that horrible sinking feeling when you\\x89Ûªve been at home on your phone for a while and you realise its been on 3G this whole time']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mislabeled = df_train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n",
    "df_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\n",
    "df_mislabeled.index.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of samples that are labeled twice. Let's create a new column where we will put new labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit',\n",
       " 'Hellfire! We don\\x89Ûªt even want to think about it or mention it so let\\x89Ûªs not do anything that leads to it #islam!',\n",
       " \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\n",
       " 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!',\n",
       " 'To fight bioterrorism sir.',\n",
       " 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE',\n",
       " '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption',\n",
       " '#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect',\n",
       " 'He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam',\n",
       " 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG',\n",
       " 'Hellfire is surrounded by desires so be careful and don\\x89Ûªt let your desires control you! #Afterlife',\n",
       " 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring',\n",
       " \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\",\n",
       " 'wowo--=== 12000 Nigerian refugees repatriated from Cameroon',\n",
       " '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4',\n",
       " 'Caution: breathing may be hazardous to your health.',\n",
       " 'I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????',\n",
       " 'that horrible sinking feeling when you\\x89Ûªve been at home on your phone for a while and you realise its been on 3G this whole time']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mislabeled = df_train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n",
    "df_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\n",
    "df_mislabeled.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target_relabeled'] = df_train['target'].copy() \n",
    "df_train.loc[df_train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"'Hellfire! We don\\x89Ûªt even want to think about it or mention it so let\\x89Ûªs not do anything that leads to it #islam!'\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == '#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'Hellfire is surrounded by desires so be careful and don\\x89Ûªt let your desires control you! #Afterlife', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'wowo--=== 12000 Nigerian refugees repatriated from Cameroon', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n",
    "df_train.loc[df_train['text'] == 'Caution: breathing may be hazardous to your health.', 'target_relabeled'] = 10\n",
    "df_train.loc[df_train['text'] == 'I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????', 'target_relabeled'] = 1\n",
    "df_train.loc[df_train['text'] == 'that horrible sinking feeling when you\\x89Ûªve been at home on your phone for a while and you realise its been on 3G this whole time', 'target_relabeled'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True)\n",
    "\n",
    "is_disaster = df_train['target'] == 1\n",
    "disaster_count = df_train[is_disaster]['target_relabeled'].count()\n",
    "not_disaster_count = df_train[~is_disaster]['target_relabeled'].count()\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train['text_cleaned'], df_train['target']), 1):\n",
    "    train_shape = df_train.loc[train_idx, 'text_cleaned'].shape\n",
    "    val_shape = df_train.loc[val_idx, 'text_cleaned'].shape\n",
    "    train_unique_keywords = df_train.loc[train_idx, 'keyword'].nunique()\n",
    "    val_unique_keywords = df_train.loc[val_idx, 'keyword'].nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoding function\n",
    "def encode(texts):\n",
    "                \n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "        text = text[:max_seq_length - 2]\n",
    "        input_sequence = ['[CLS]'] + text + ['[SEP]']\n",
    "        pad_len = max_seq_length - len(input_sequence)\n",
    "\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_seq_length\n",
    "\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "# Define the model\n",
    "def build_model():\n",
    "    \n",
    "    input_word_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "    input_mask = Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "    segment_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name='segment_ids')    \n",
    "    \n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])   \n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    optimizer = SGD(learning_rate=lr, momentum=0.8)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the training function\n",
    "def train_model(X, tokenizer, max_seq_length, lr, epochs, batch_size):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    models = []\n",
    "    scores = {}\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(skf.split(X['text_cleaned'], X['target'])):\n",
    "        \n",
    "        print('\\nFold {}\\n'.format(fold))\n",
    "    \n",
    "        X_trn_encoded = encode(X.loc[trn_idx, 'text_cleaned'])\n",
    "        y_trn = X.loc[trn_idx, 'target']\n",
    "        X_val_encoded = encode(X.loc[val_idx, 'text_cleaned'])\n",
    "        y_val = X.loc[val_idx, 'target']\n",
    "    \n",
    "        # Callbacks\n",
    "        class ClassificationReport(Callback):\n",
    "\n",
    "            def __init__(self, train_data, validation_data):\n",
    "                super().__init__()\n",
    "\n",
    "                self.X_train, self.y_train = train_data\n",
    "                self.X_val, self.y_val = validation_data\n",
    "\n",
    "                self.train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "----------------------------------\n",
      "\n",
      "Fold 0\n",
      "\n",
      " 68/119 [================>.............] - ETA: 7:47 - loss: 0.5447 - accuracy: 0.7293"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "max_seq_length = 128\n",
    "lr = 0.001\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Train the model\n",
    "model = build_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch {}/{}'.format(epoch+1, epochs))\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train['text_cleaned'], df_train['target'])):\n",
    "        \n",
    "        print('\\nFold {}\\n'.format(fold))\n",
    "        \n",
    "        X_trn_encoded = encode(df_train.loc[trn_idx, 'text_cleaned'])\n",
    "        y_trn = df_train.loc[trn_idx, 'target']\n",
    "        X_val_encoded = encode(df_train.loc[val_idx, 'text_cleaned'])\n",
    "        y_val = df_train.loc[val_idx, 'target']\n",
    "        \n",
    "        history = model.fit(\n",
    "            [X_trn_encoded[0], X_trn_encoded[1], X_trn_encoded[2]], y_trn,\n",
    "            validation_data=([X_val_encoded[0], X_val_encoded[1], X_val_encoded[2]], y_val),\n",
    "            epochs=1,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate on the validation set\n",
    "        y_val_pred = model.predict([X_val_encoded[0], X_val_encoded[1], X_val_encoded[2]])\n",
    "        y_val_pred = (y_val_pred > 0.5).astype(int)\n",
    "        \n",
    "        val_precision = precision_score(y_val, y_val_pred)\n",
    "        val_recall = recall_score(y_val, y_val_pred)\n",
    "        val_f1 = f1_score(y_val, y_val_pred)\n",
    "        \n",
    "        print('\\nValidation Precision: {:.4f}, Recall: {:.4f}, F1: {:.4f}\\n'.format(val_precision, val_recall, val_f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
